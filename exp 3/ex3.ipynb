{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 3: WAP to evaluate the performance of perceptron with linear and sigmoid activation functions for a regression and binary classification problem respectively\n",
    "\n",
    "The code implements a three-layer neural network using TensorFlow (without Keras) to classify handwritten digits from the MNIST dataset. It follows a structured approach involving:\n",
    "\n",
    "1. Data Preprocessing:\n",
    "   - Normalizing pixel values to [0, 1].\n",
    "   - Flattening the images from 28x28 to 784 dimensions.\n",
    "   - One-hot encoding the labels for classification.\n",
    "\n",
    "2. Model Architecture:\n",
    "   - Input Layer: 784 neurons.\n",
    "   - Hidden Layer 1: 128 neurons, Activation Function: ReLU.\n",
    "   - Hidden Layer 2: 64 neurons, Activation Function: ReLU.\n",
    "   - Output Layer: 10 neurons (for classes 0-9).\n",
    "\n",
    "3. Training Process:\n",
    "   - Forward Propagation: Matrix multiplication followed by activation functions.\n",
    "   - Loss Calculation: Categorical Cross-Entropy.\n",
    "   - Backpropagation: Using TensorFlow's automatic differentiation.\n",
    "   - Optimization: Adam optimizer with a learning rate scheduler.\n",
    "   - Regularization: Batch normalization and dropout applied to prevent overfitting.\n",
    "\n",
    "4. Evaluation:\n",
    "   - Computes accuracy on the test dataset after training.\n",
    "\n",
    "The code demonstrates efficient training using TensorFlow's low-level API with improvements for better performance and stability. It prints training progress for each epoch and displays final test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2us/step\n",
      "Epoch 1, Loss: 0.4589, Train Accuracy: 86.27%, Test Accuracy: 86.85%\n",
      "Epoch 2, Loss: 0.2187, Train Accuracy: 89.17%, Test Accuracy: 89.02%\n",
      "Epoch 3, Loss: 0.1705, Train Accuracy: 90.25%, Test Accuracy: 90.33%\n",
      "Epoch 4, Loss: 0.1431, Train Accuracy: 88.49%, Test Accuracy: 88.26%\n",
      "Epoch 5, Loss: 0.1260, Train Accuracy: 88.41%, Test Accuracy: 88.30%\n",
      "Epoch 6, Loss: 0.1137, Train Accuracy: 90.28%, Test Accuracy: 89.31%\n",
      "Epoch 7, Loss: 0.1026, Train Accuracy: 90.20%, Test Accuracy: 89.24%\n",
      "Epoch 8, Loss: 0.0970, Train Accuracy: 90.18%, Test Accuracy: 89.28%\n",
      "Epoch 9, Loss: 0.0909, Train Accuracy: 90.40%, Test Accuracy: 90.07%\n",
      "Epoch 10, Loss: 0.0858, Train Accuracy: 89.97%, Test Accuracy: 89.54%\n",
      "Epoch 11, Loss: 0.0816, Train Accuracy: 90.90%, Test Accuracy: 89.86%\n",
      "Epoch 12, Loss: 0.0751, Train Accuracy: 92.26%, Test Accuracy: 90.82%\n",
      "Epoch 13, Loss: 0.0747, Train Accuracy: 92.27%, Test Accuracy: 91.19%\n",
      "Epoch 14, Loss: 0.0722, Train Accuracy: 91.65%, Test Accuracy: 90.29%\n",
      "Epoch 15, Loss: 0.0677, Train Accuracy: 91.96%, Test Accuracy: 90.97%\n",
      "Epoch 16, Loss: 0.0648, Train Accuracy: 91.29%, Test Accuracy: 90.11%\n",
      "Epoch 17, Loss: 0.0620, Train Accuracy: 92.74%, Test Accuracy: 91.62%\n",
      "Epoch 18, Loss: 0.0624, Train Accuracy: 92.75%, Test Accuracy: 91.61%\n",
      "Epoch 19, Loss: 0.0571, Train Accuracy: 93.20%, Test Accuracy: 91.99%\n",
      "Epoch 20, Loss: 0.0553, Train Accuracy: 92.58%, Test Accuracy: 91.33%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess dataset\n",
    "x_train, x_test = x_train.reshape(-1, 28*28).astype(np.float32) / 255.0, x_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
    "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n",
    "\n",
    "# Define network parameters\n",
    "input_size = 784\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Initialize weights and biases\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "weights = {\n",
    "    'W1': tf.Variable(initializer([input_size, hidden_size1])),\n",
    "    'W2': tf.Variable(initializer([hidden_size1, hidden_size2])),\n",
    "    'W3': tf.Variable(initializer([hidden_size2, output_size])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden_size1])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden_size2])),\n",
    "    'b3': tf.Variable(tf.zeros([output_size])),\n",
    "}\n",
    "\n",
    "# Define feed-forward function with Batch Normalization and Dropout\n",
    "\n",
    "def forward_propagation(x, training=True):\n",
    "    z1 = tf.matmul(x, weights['W1']) + biases['b1']\n",
    "    a1 = tf.nn.relu(tf.keras.layers.BatchNormalization()(z1, training=training))\n",
    "    a1 = tf.nn.dropout(a1, rate=0.2)\n",
    "\n",
    "    z2 = tf.matmul(a1, weights['W2']) + biases['b2']\n",
    "    a2 = tf.nn.relu(tf.keras.layers.BatchNormalization()(z2, training=training))\n",
    "    a2 = tf.nn.dropout(a2, rate=0.2)\n",
    "\n",
    "    z3 = tf.matmul(a2, weights['W3']) + biases['b3']\n",
    "    return z3  # No softmax here; handled by loss function\n",
    "\n",
    "# Loss function\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Optimizer with learning rate scheduler\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "# Training step function\n",
    "\n",
    "def train_step(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = forward_propagation(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, logits)\n",
    "    gradients = tape.gradient(loss, list(weights.values()) + list(biases.values()))\n",
    "    optimizer.apply_gradients(zip(gradients, list(weights.values()) + list(biases.values())))\n",
    "    return loss\n",
    "\n",
    "# Evaluation function\n",
    "\n",
    "def evaluate(x_data, y_data):\n",
    "    logits = forward_propagation(x_data, training=False)\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    true_labels = tf.argmax(y_data, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, true_labels), tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "num_batches = x_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x_train[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_y = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "        avg_loss += loss / num_batches\n",
    "\n",
    "    train_acc = evaluate(x_train, y_train)\n",
    "    test_acc = evaluate(x_test, y_test)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss.numpy():.4f}, Train Accuracy: {train_acc.numpy() * 100:.2f}%, Test Accuracy: {test_acc.numpy() * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
